{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Recommender Systems</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate some gradient-based algorithms on the very well known matrix factorization problem which is the most prominent approach for build a Recommender Systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "The problem of matrix factorization for collaborative filtering captured much attention, especially after the [Netflix prize](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf). The premise behind this approach is to approximate a large rating matrix $R$ with the multiplication of two low-dimensional factor matrices $P$ and $Q$, i.e. $R \\approx \\hat{R} = P^TQ$, that model respectively users and items in some latent space. For instance, matrix $R$ has dimension $m \\times  n$ where $m$ and $n$ are restrictively the number of users and items, both large; while $P$ has size $m \\times  k$ and contains user information in a latent space of size $k<<m,n$, $Q$ has size $n\\times k$ and contains item information in the same latent space of size $k << m,n$. Typical values for $m, n$ are $10^6$ while $k$ is in the tens.\n",
    "\n",
    "For a pair of user and item $(u_i,i_j)$ for which a rating $r_{ij}$ exists, a common approach approach is based on the minimization of the $\\ell_2$-regularized quadratic error:\n",
    "$$  \\ell_{u_i,i_j}(P,Q)= \\left(r_{ij} - p_{i}^{\\top}q_{j}\\right)^2 + \\lambda(|| p_{i} ||^{2} + || q_{j} ||^2 )  $$\n",
    "where $p_i$ is the column vector composed of the $i$-th line of $P$ and  $\\lambda\\geq 0$ is a regularization parameter. The whole matrix factorization problem thus writes\n",
    "$$ \\min_{P,Q} \\sum_{i,j : r_{ij} \\text{exists}}  \\ell_{u_i,i_j}(P,Q). $$\n",
    "Note that the error $ \\ell_{u_i,i_j}(P,Q)$ depends only on $P$ and $Q$ through $p_{i}$ and $q_{j}$; however, item $i_j$ may also be rated by user $u_{i'}$ so that the optimal factor $q_{j}$ depends on both $p_{i}$ and $p_{i'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc.stop()\n",
    "# set up spark environment (Using Spark Local Mode)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "conf.setAppName(\"MSIAM part III - Matrix Factorization\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1193, 5.0)\n",
      "We have 1000209 ratings from 6040 users on 3706 movies.\n",
      "\n",
      "We have 6040 users, 3952 movies and the rating matrix has 4.190221 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parseRating(line):\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "\n",
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"../data/movielens/medium/\"\n",
    "\n",
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir + \"ratings.dat\").map(parseRating).setName(\"ratings\").cache()\n",
    "\n",
    "numRatings = ratingsRDD.count()\n",
    "print(ratingsRDD.first())\n",
    "numUsers = ratingsRDD.map(lambda r: r[0]).distinct().count()\n",
    "numMovies = ratingsRDD.map(lambda r: r[1]).distinct().count()\n",
    "print(\"We have %d ratings from %d users on %d movies.\\n\" % (numRatings, numUsers, numMovies))\n",
    "\n",
    "N = ratingsRDD.map(lambda r: r[0]).max()\n",
    "M = ratingsRDD.map(lambda r: r[1]).max()\n",
    "matrixSparsity = float(numRatings)/float(M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % (N, M, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Split (ramdomly) the dataset into training versus testing sample. We learn over 70% (for example) of the users, we test over the rest.\n",
    "\n",
    "> Define a routine that returns the predicted rating from factor matrices. Form a RDD with the following elements `(i,j,true rating,predicted rating)`. \n",
    "\n",
    "> Define a routine that returns the Mean Square Error (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, array([1, 1]))]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainTestSplit = ratingsRDD.randomSplit(weights=[0.70, 0.30],seed=1)\n",
    "train = trainTestSplit[0]\n",
    "test = trainTestSplit[1]\n",
    "\n",
    "# P -> ((i), ith row of P)\n",
    "# Q -> ((j), jth col of Q)\n",
    "# R -> ((i,j), true_rating_ij)\n",
    "def predict_rating(P, Q, R, i, j):\n",
    "    #print(P.lookup(i))\n",
    "    #print(Q.lookup(j))\n",
    "    predicted_rating = np.dot(P.lookup(i)[0], Q.lookup(j)[0])\n",
    "    return sc.parallelize([((i,j), R.lookup((i,j))[0], predicted_rating)])\n",
    "    \n",
    "# R_hat -> ((i, j), true_rating, predicted_rating)\n",
    "def mean_square_error(R_hat):\n",
    "    return ((R_hat[1] - R_hat[2]) ** 2)\n",
    "\n",
    "P = sc.parallelize([(1, np.array([1,1])), (2, np.array([2,2]))])\n",
    "Q = sc.parallelize([(1, np.array([1,1])), (2, np.array([2,2]))])\n",
    "R = sc.parallelize([((1,1), 1), ((2,2), 1), ((1,2), 1), ((2,1), 1)])\n",
    "\n",
    "print(P.take(1))\n",
    "\n",
    "print(mean_square_error(predict_rating(P, Q, R, 2, 1).take(1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implementing a gradient algorithm in `Python` on the training set.  Taking a step size (learning rate) $\\gamma=0.001$ and stop after a specified number of iterations. Investigate the latent space size with $K=2$).\n",
    "\n",
    "\n",
    "Stochastic Gradient Descent (SGD) simply does away with the expectation in the update and computes the gradient of the parameters using only a single or a few training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-10abdd0bbad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def prediction(P,Q):\n",
    "    return np.dot(P,Q.T)\n",
    "\n",
    "def rmse(R,Q,P):\n",
    "    return np.sum(((R - prediction(P,Q)))**2)/len(R[R > 0])\n",
    "\n",
    "train_errors = []\n",
    "# PYTHON\n",
    "# Beta is alpha/2\n",
    "def full_grad_algo(R, P, Q, K, steps=10, alpha=0.001, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        print(step)\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        train_errors.append(e)\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "\n",
    "K = 2\n",
    "\n",
    "inputV_filepath = movieLensHomeDir + \"ratings.dat\"\n",
    "from scipy import sparse\n",
    "def CreateMatrix(num_users, num_movies) :\n",
    "    with open(inputV_filepath, 'r') as f_in :\n",
    "        V = sparse.lil_matrix((num_users, num_movies))\n",
    "        for line in f_in :\n",
    "            line = line.rstrip()\n",
    "            eachline = line.split(\"::\")\n",
    "            for i in range(3) :\n",
    "                eachline[i] = int(eachline[i])\n",
    "            V[eachline[0] - 1, eachline[1] - 1] = eachline[2]\n",
    "    return V\n",
    "\n",
    "V = CreateMatrix(N,M)\n",
    "R = V.toarray()\n",
    "\n",
    "steps = 5\n",
    "\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "\n",
    "nP, nQ = full_grad_algo(R, P, Q, K, steps)\n",
    "nR = np.dot(nP, nQ.T)\n",
    "\n",
    "print(train_errors)\n",
    "\n",
    "plt.plot(range(steps), train_errors, label='Training Data');\n",
    "plt.title('SGD-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('MSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the ALS method already implemented in MLlib as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::1193::5::978300760\n",
      "1000209\n",
      "3.58156445303\n",
      "6040\n",
      "699782\n",
      "300427\n",
      "(2, array('d', [-0.8554146885871887, -0.29376375675201416, 0.17143267393112183, -0.2958856523036957, -0.585989773273468]))\n",
      "(2, array('d', [-2.3156867027282715, -1.8215765953063965, 0.15810754895210266, -0.27069902420043945, -1.3080871105194092]))\n",
      "Root Mean Square Error on testing data : 0.775564392921\n",
      "Time taken on 2 cores : 137.067409 seconds.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "movielens = sc.textFile(\"../data/movielens/medium/ratings.dat\")\n",
    "\n",
    "print(movielens.first()) #u'196\\t242\\t3\\t881250949'\n",
    "print(movielens.count()) #100000\n",
    "\n",
    "#Clean up the data by splitting it\n",
    "#Movielens readme says the data is split by tabs and\n",
    "#is user product rating timestamp\n",
    "clean_data = movielens.map(lambda x:x.split(\"::\"))\n",
    "\n",
    "#As an example, extract just the ratings to its own RDD\n",
    "#rate.first() is 3\n",
    "rate = clean_data.map(lambda y: int(y[2]))\n",
    "print(rate.mean()) #Avg rating is 3.52986\n",
    "\n",
    "#Extract just the users\n",
    "users = clean_data.map(lambda y: int(y[0]))\n",
    "print(users.distinct().count()) #943 users\n",
    "\n",
    "#You don't have to extract data to its own RDD\n",
    "#This command counts the distinct movies\n",
    "#There are 1,682 movies\n",
    "clean_data.map(lambda y: int(y[1])).distinct().count() \n",
    "\n",
    "# AVERAGE RATING - 3.58\n",
    "\n",
    "#Need to import three functions / objects from the MLlib\n",
    "from pyspark.mllib.recommendation\\\n",
    "    import ALS,MatrixFactorizationModel, Rating\n",
    "\n",
    "#We'll need to map the movielens data to a Ratings object \n",
    "#A Ratings object is made up of (user, item, rating)\n",
    "mls = movielens.map(lambda l: l.split('::'))\n",
    "ratings = mls.map(lambda x: Rating(int(x[0]),\\\n",
    "    int(x[1]), float(x[2])))\n",
    "\n",
    "#Need a training and test set\n",
    "train, test = ratings.randomSplit([0.7,0.3],7856)\n",
    "\n",
    "print(train.count()) #70,005\n",
    "print(test.count()) #29,995\n",
    "\n",
    "#Need to cache the data to speed up training\n",
    "train.cache()\n",
    "test.cache()\n",
    "\n",
    "#Setting up the parameters for ALS\n",
    "rank = 5 # Latent Factors to be made\n",
    "numIterations = 10 # Times to repeat process\n",
    "#Create the model on the training data\n",
    "model = ALS.train(train, rank, numIterations)\n",
    "\n",
    "#Examine the latent features for one product\n",
    "print(model.productFeatures().first())\n",
    "#(12, array('d', [-0.29417645931243896, 1.8341970443725586, \n",
    "    #-0.4908868968486786, 0.807500958442688, -0.8945541977882385]))\n",
    "\n",
    "#Examine the latent features for one user\n",
    "print(model.userFeatures().first())\n",
    "#(12, array('d', [1.1348751783370972, 2.397622585296631,\n",
    "    #-0.9957215785980225, 1.062819480895996, 0.4373367130756378]))\n",
    "\n",
    "# For Product X, Find N Users to Sell To\n",
    "model.recommendUsers(242,100)\n",
    "\n",
    "# For User Y Find N Products to Promote\n",
    "model.recommendProducts(196,10)\n",
    "\n",
    "#Predict Single Product for Single User\n",
    "model.predict(196, 242)\n",
    "\n",
    "# Predict Multi Users and Multi Products\n",
    "# Pre-Processing\n",
    "pred_input = train.map(lambda x:(x[0],x[1]))   \n",
    "\n",
    "# Lots of Predictions\n",
    "#Returns Ratings(user, item, prediction)\n",
    "pred = model.predictAll(pred_input) \n",
    "\n",
    "#Get Performance Estimate\n",
    "#Organize the data to make (user, product) the key)\n",
    "true_reorg = train.map(lambda x:((x[0],x[1]), x[2]))\n",
    "pred_reorg = pred.map(lambda x:((x[0],x[1]), x[2]))\n",
    "\n",
    "#Do the actual join\n",
    "true_pred = true_reorg.join(pred_reorg)\n",
    "\n",
    "#Need to be able to square root the Mean-Squared Error\n",
    "from math import sqrt\n",
    "\n",
    "MSE = true_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "RMSE = sqrt(MSE)#Results in 0.7629908117414474\n",
    "\n",
    "#Test Set Evaluation\n",
    "#More dense, but nothing we haven't done before\n",
    "test_input = test.map(lambda x:(x[0],x[1])) \n",
    "pred_test = model.predictAll(test_input)\n",
    "test_reorg = test.map(lambda x:((x[0],x[1]), x[2]))\n",
    "pred_reorg = pred_test.map(lambda x:((x[0],x[1]), x[2]))\n",
    "test_pred = test_reorg.join(pred_reorg)\n",
    "test_MSE = test_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "test_RMSE = sqrt(test_MSE)#1.0145549956596238\n",
    "endTime = datetime.now()\n",
    "diff = endTime - startTime\n",
    "print(\"Root Mean Square Error on testing data : \" + str(test_MSE))\n",
    "print(\"Time taken on 2 cores : \" + str(diff.total_seconds()) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>Examples taken from the course [Convex and Distributed Optimization](http://www.iutzeler.org/CDO/) (2016).</i></small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
