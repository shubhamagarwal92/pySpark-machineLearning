{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark\n",
    "__-Shubham Agarwal__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created as an experiment with PySpark with the Jupyter notebook.\n",
    "It is assumed that Jupyter (iPython) and Spark have already been installed.\n",
    "To use PySpark with Jupyter notebook, you might have to update bash_profile or bashrc as\n",
    "\n",
    "```\n",
    "export SPARK_HOME=\"/Users/username/spark-version\"\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH\n",
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-spark_version-src.zip:$PYTHONPATH\n",
    "```\n",
    "\n",
    "Exporting variable PYSPARK_SUBMIT_ARGS can create issues with Jupyter. I would rather not recommend this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by defining Spark Context and proceed with a basic example of word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up spark environment\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"\")\n",
    "# sc = SparkContext(\"local[2*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 5\n"
     ]
    }
   ],
   "source": [
    "# Basic example\n",
    "words = sc.parallelize([\"scala\",\"java\",\"hadoop\",\"spark\",\"python\"])\n",
    "print \"Number of words: \" + str(words.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the interface by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook, we will be experimenting with the [MovieLens](https://movielens.org) dataset. Assuming data in the `/data/movielens/medium ` directory. \n",
    "\n",
    "We will use two files from this dataset: `ratings.dat` and `movies.dat`. All ratings are contained in the file `ratings.dat` and are in the following format:\n",
    "```\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "```\n",
    "The movie information is in the file `movies.dat` and is in the following format:\n",
    "\n",
    "```\n",
    "MovieID::Title::Genres\n",
    "```\n",
    "\n",
    "Let's start with the data. Loading the dataset:\n",
    "- [MovieLens 1M Dataset](http://grouplens.org/datasets/movielens/1m/ml-1m.zip) - 1 million ratings from 6000 users on 4000 movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"../data/movielens/medium/\"\n",
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../\"]).decode(\"utf8\"))\n",
    "moviesPath = movieLensHomeDir + \"movies.dat\"\n",
    "ratingsPath = movieLensHomeDir + \"ratings.dat\"\n",
    "usersPath = movieLensHomeDir + \"users.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining two functions `parseRating` and `parseMovie` that parse a rating and a movie record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseRating(line):\n",
    "    \"\"\" Parse a rating record in MovieLens format UserID::MovieID::Rating::Timestamp\n",
    "    Args:\n",
    "        line (str): a line in the ratings dataset in the form of UserID::MovieID::Rating::Timestamp\n",
    "    Returns:\n",
    "        tuple: (UserID, MovieID, Rating)\n",
    "    \"\"\"\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "def parseMovie(line):\n",
    "    \"\"\" Parse a movie record in MovieLens format MovieID::Title::Genres\n",
    "    Args:\n",
    "        entry (str): a line in the movies dataset in the form of MovieID::Title::Genres\n",
    "    Returns:\n",
    "        tuple: (MovieID, Title, Genres)\n",
    "    \"\"\"\n",
    "    fields = line.split(\"::\")\n",
    "    return int(fields[0]), fields[1], fields[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# movies is an RDD of (movieID, title, genre)\n",
    "moviesRDD = sc.textFile(movieLensHomeDir + \"movies.dat\").map(parseMovie).setName(\"movies\")\n",
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir + \"ratings.dat\").map(parseRating).setName(\"ratings\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ - In these lines of code, we are creating the `moviesRDD` and `ratingsRDD` variables (technically RDDs) and we are pointing to files (on your local PC). Spark’s lazy nature means that it doesn’t automatically compile your code. Instead, it waits for some sort of action occurs that requires some calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000209\n",
      "3883\n",
      "(1, 1193, 5.0)\n",
      "[(1, u'Toy Story (1995)', u\"Animation|Children's|Comedy\"), (2, u'Jumanji (1995)', u\"Adventure|Children's|Fantasy\"), (3, u'Grumpier Old Men (1995)', u'Comedy|Romance'), (4, u'Waiting to Exhale (1995)', u'Comedy|Drama'), (5, u'Father of the Bride Part II (1995)', u'Comedy')]\n"
     ]
    }
   ],
   "source": [
    "numRatings = ratingsRDD.count()\n",
    "numMovies  = moviesRDD.count()\n",
    "#numUsers   = ratingsRDD.\n",
    "print(numRatings)\n",
    "print(numMovies)\n",
    "print (ratingsRDD.first())\n",
    "print(moviesRDD.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Define two new RDDs containing only the movies for genre _Comedy_ and all movies that have _Comedy_ among other genres.<br/>\n",
    ">Use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter\">`filter`</a> function which return a new RDD containing only the elements that satisfy a predicate.<br/>\n",
    ">Use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.subtract\">`subtract`</a> function to retreive the movies that have  _Comedy_ in their genres but not only (That is the elements of the second RDD minus the ones in the first). Count them and exhibit a few of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521\n",
      "1200\n",
      "679\n",
      "[(1, u'Toy Story (1995)', u\"Animation|Children's|Comedy\"), (3, u'Grumpier Old Men (1995)', u'Comedy|Romance'), (4, u'Waiting to Exhale (1995)', u'Comedy|Drama'), (5, u'Father of the Bride Part II (1995)', u'Comedy'), (7, u'Sabrina (1995)', u'Comedy|Romance')]\n"
     ]
    }
   ],
   "source": [
    "def hasComedy(movie):\n",
    "    genres = movie[2].split(\"|\")\n",
    "    return \"Comedy\" in genres\n",
    "def onlyComedy(movie):\n",
    "    genres = movie[2].split(\"|\")\n",
    "    if len(genres)==1 and \"Comedy\" in genres:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "onlyComedyRDD = moviesRDD.filter(lambda movie : onlyComedy(movie))\n",
    "ComedyRDD = moviesRDD.filter(lambda movie : hasComedy(movie))\n",
    "print(onlyComedyRDD.count())\n",
    "print(ComedyRDD.count())\n",
    "notOnlyComedyRDD = ComedyRDD.subtract(onlyComedyRDD)\n",
    "print(notOnlyComedyRDD.count())\n",
    "print(ComedyRDD.take(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Investigate the different movies genres. Warning: Multiples genres should not be seen as new genres! For this:\n",
    "* separate the genres by delimiter '|' using  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap\">`flatMap`</a>\n",
    "* use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct\">`distinct`</a> function which return a new RDD containing the distinct elements in this RDD.\n",
    "\n",
    ">Count the number of different genres and print them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "genresRDD = moviesRDD.flatMap(lambda movie : movie[2].split('|')).distinct()\n",
    "print(genresRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average of all of the ratings (use the mean built-in function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58156445303\n"
     ]
    }
   ],
   "source": [
    "avg=ratingsRDD.map(lambda r : (r[2])).mean()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Get the rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 107557), (4.0, 348971), (1.0, 56174), (3.0, 261197), (5.0, 226310)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "frequencyRDD = ratingsRDD.map(lambda r : (r[2], 1)).reduceByKey(add)\n",
    "print(frequencyRDD.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>Examples taken from the course Convex and Distributed Optimization (2016).</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
